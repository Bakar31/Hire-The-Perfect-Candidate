{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# required libraries\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from pdfminer import high_level\r\n",
    "\r\n",
    "train = pd.read_csv('dataset/train.csv')\r\n",
    "test = pd.read_csv('dataset/test.csv')\r\n",
    "\r\n",
    "#paths\r\n",
    "train_path = \"dataset/trainResumes/\"\r\n",
    "test_path = \"dataset/testResumes/\"\r\n",
    "\r\n",
    "# epty list for resumes text\r\n",
    "train_resumes = []\r\n",
    "test_resumes = []\r\n",
    "\r\n",
    "# ids\r\n",
    "ids = list(train.CandidateID)\r\n",
    "test_ids = list(test.CandidateID)\r\n",
    "\r\n",
    "# pdf2string\r\n",
    "def pdf2string_train(path, ids, resumes):\r\n",
    "    for i in ids:\r\n",
    "        main_path = path+i+'.pdf'\r\n",
    "        text = high_level.extract_text(main_path)\r\n",
    "        str_list = text.split()\r\n",
    "        str_list = str_list[:]\r\n",
    "        string = ' '.join(str_list)\r\n",
    "        resumes.append(string)\r\n",
    "        \r\n",
    "\r\n",
    "def pdf2string_test(path, test_ids, resumes):\r\n",
    "    for i in test_ids:\r\n",
    "        main_path = path+i+'.pdf'\r\n",
    "        text = high_level.extract_text(main_path)\r\n",
    "        str_list = text.split()\r\n",
    "        str_list = str_list[:]\r\n",
    "        string = ' '.join(str_list)\r\n",
    "        resumes.append(string)\r\n",
    "\r\n",
    "\r\n",
    "pdf2string_train(train_path, ids, train_resumes)\r\n",
    "pdf2string_test(test_path,  test_ids, test_resumes)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import nltk\r\n",
    "import spacy\r\n",
    "import string\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.corpus import wordnet\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "\r\n",
    "train_resumes_lower = []\r\n",
    "for resume in train_resumes:\r\n",
    "    train_resumes_lower.append(resume.lower())\r\n",
    "\r\n",
    "test_resumes_lower = []\r\n",
    "for resume in test_resumes:\r\n",
    "    test_resumes_lower.append(resume.lower())\r\n",
    "\r\n",
    "PUNCT_TO_REMOVE = string.punctuation\r\n",
    "def remove_punctuation(text):\r\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\r\n",
    "\r\n",
    "train_punc_removed = []\r\n",
    "for resume in train_resumes_lower:\r\n",
    "    punc_removed = remove_punctuation(resume)\r\n",
    "    train_punc_removed.append(punc_removed)\r\n",
    "\r\n",
    "test_punc_removed = []\r\n",
    "for resume in test_resumes_lower:\r\n",
    "    punc_removed = remove_punctuation(resume)\r\n",
    "    test_punc_removed.append(punc_removed)\r\n",
    "\r\n",
    "STOPWORDS = set(stopwords.words('english'))\r\n",
    "def remove_stopwords(text):\r\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\r\n",
    "\r\n",
    "train_stopwords_removed = []\r\n",
    "for resume in train_punc_removed:\r\n",
    "    stopwords_removed = remove_stopwords(resume)\r\n",
    "    train_stopwords_removed.append(stopwords_removed)\r\n",
    "\r\n",
    "test_stopwords_removed = []\r\n",
    "for resume in test_punc_removed:\r\n",
    "    stopwords_removed = remove_stopwords(resume)\r\n",
    "    test_stopwords_removed.append(stopwords_removed)\r\n",
    "\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\r\n",
    "def lemmatize_words(text):\r\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\r\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\r\n",
    "\r\n",
    "train_lemma = []\r\n",
    "for resume in train_stopwords_removed:\r\n",
    "    lemma = lemmatize_words(resume)\r\n",
    "    train_lemma.append(lemma)\r\n",
    "\r\n",
    "test_lemma = []\r\n",
    "for resume in test_stopwords_removed:\r\n",
    "    lemma = lemmatize_words(resume)\r\n",
    "    test_lemma.append(lemma)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_lemma[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'l n r e w f r e h e r work experience dictis make intern trainee jan 2020 apr 2020 responsible perform help decision executive summary fresher strong statistical analytic capability someone drive passion problem solve though civil engineering background always fascinate data machine learn evolve iit personal skill data analyst data mining data visualization machine learn linear regression statistical model predictive modeling sql server oracle python project extracurriculars data preprocessing python data visualization power bi academic profile railway signal determiner use relay weight system 2020 btechcivil garodia institute technosciences'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "train = pd.read_csv('dataset/train.csv')\r\n",
    "test = pd.read_csv('dataset/test.csv')\r\n",
    "\r\n",
    "train_df = pd.concat([train, pd.DataFrame(train_lemma, columns=['resumes'])], axis = 1)\r\n",
    "test_df = pd.concat([test, pd.DataFrame(test_lemma, columns=['resumes'])], axis = 1)\r\n",
    "\r\n",
    "print(train_df.head())\r\n",
    "print(test_df.head())\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer(max_features=10000, \r\n",
    "                        strip_accents='unicode', \r\n",
    "                        analyzer='word',\r\n",
    "                        lowercase=False,\r\n",
    "                        ngram_range=(1, 1), \r\n",
    "                        stop_words = 'english')\r\n",
    "\r\n",
    "tfidf_matrix_train = tfidf.fit_transform(train_df['resumes'])\r\n",
    "tfidf_matrix_test = tfidf.transform(test_df['resumes'])\r\n",
    "print(tfidf_matrix_train.shape)\r\n",
    "print(tfidf_matrix_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     CandidateID  Match Percentage  \\\n",
      "0  candidate_011             13.60   \n",
      "1  candidate_113             36.63   \n",
      "2  candidate_123             54.93   \n",
      "3  candidate_012             41.46   \n",
      "4  candidate_002             48.91   \n",
      "\n",
      "                                             resumes  \n",
      "0  l n r e w f r e h e r work experience dictis m...  \n",
      "1  ellie mackey f r e h e r n e r n executive pro...  \n",
      "2  f e l x w n n fresher skills project activites...  \n",
      "3  jimmy gartner n g e r professional profile emp...  \n",
      "4  n q u r associate analyst skill certify data a...  \n",
      "     CandidateID                                            resumes\n",
      "0  candidate_014  grace bailry c h n e l e r n n g e v e l p e n...\n",
      "1  candidate_098  l e l e u n software engineer skill assistant ...\n",
      "2  candidate_075  keiron pavard e n g n e e r personal profile w...\n",
      "3  candidate_016  e l r n c e n c e j r work experience na acade...\n",
      "4  candidate_131  zachary perez n e r n p r f l e k l l good kno...\n",
      "(90, 1873)\n",
      "(60, 1873)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import xgboost as XGB"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "y = train_df['Match Percentage']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.model_selection import GridSearchCV\r\n",
    "xgb_grid = {'learning_rate': [0.1, 0.3, 0.2, 0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.005, 0.006, 0.007, 0.0075, 0.008],\r\n",
    "            'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200],\r\n",
    "            'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 20]}\r\n",
    "\r\n",
    "\r\n",
    "xgb = GridSearchCV(XGB.XGBRegressor(),\r\n",
    "                    param_grid = xgb_grid,\r\n",
    "                    cv=2,\r\n",
    "                    verbose=True)\r\n",
    "xgb.fit(tfidf_matrix_train, y)\r\n",
    "xgb.best_params_"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 2 folds for each of 2184 candidates, totalling 4368 fits\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "\r\n",
    "xgb = XGB.XGBRegressor(learning_rate=0.005, \r\n",
    "                        n_estimators=700, \r\n",
    "                        objective='reg:squarederror', \r\n",
    "                        max_depth=8, \r\n",
    "                        reg_lambda = 1.3,\r\n",
    "                        gamma = 1,\r\n",
    "                        min_child_weight =1.5,\r\n",
    "                        max_delta_step = 100,\r\n",
    "                        random_state = 31).fit(tfidf_matrix_train, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "from lightgbm import LGBMRegressor\r\n",
    "lgbm = LGBMRegressor(num_leaves=31,\r\n",
    "                    learning_rate = 0.01,\r\n",
    "                    n_estimators = 1000,\r\n",
    "                    reg_lambda = 2.5,\r\n",
    "                    reg_alpha = 2,\r\n",
    "                    random_state=31).fit(tfidf_matrix_train, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "def submission(model, test_sentences):\r\n",
    "    test1 = pd.read_csv('dataset/test.csv')\r\n",
    "    preds = model.predict(test_sentences)\r\n",
    "    prediction = pd.DataFrame(preds, columns = ['Match Percentage'])\r\n",
    "    sub_df = pd.concat([test1, prediction], axis = 1)\r\n",
    "    return sub_df\r\n",
    "\r\n",
    "sub = submission(lgbm, tfidf_matrix_test)\r\n",
    "sub.to_csv('submission file/lgbm sub.csv')\r\n",
    "print(sub.head(10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     CandidateID  Match Percentage\n",
      "0  candidate_014         23.700393\n",
      "1  candidate_098         32.999942\n",
      "2  candidate_075         38.796772\n",
      "3  candidate_016         35.225443\n",
      "4  candidate_131         33.061261\n",
      "5  candidate_056         35.019762\n",
      "6  candidate_141         48.173307\n",
      "7  candidate_044         55.757834\n",
      "8  candidate_029         31.043669\n",
      "9  candidate_120         36.911600\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "print(sub.head(10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     CandidateID  Match Percentage\n",
      "0  candidate_014         23.700393\n",
      "1  candidate_098         32.999942\n",
      "2  candidate_075         38.796772\n",
      "3  candidate_016         35.225443\n",
      "4  candidate_131         33.061261\n",
      "5  candidate_056         35.019762\n",
      "6  candidate_141         48.173307\n",
      "7  candidate_044         55.757834\n",
      "8  candidate_029         31.043669\n",
      "9  candidate_120         36.911600\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "0bd2b4fb9f3446da4d8f55e0ab2f46c8924547cca4f6669133edbde87e094ed0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}